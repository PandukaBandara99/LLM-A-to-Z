{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "VBZDi1dwyJtM",
        "outputId": "1a458bd4-6932-44b6-c7ea-537f692c52f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "5272eef5ae9b41c398d0402b7927b7f5",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Uncomment the following line to install the gensim library\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkv2bnc2Mh1y"
      },
      "source": [
        "# Import trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCPkvuHFy16u",
        "outputId": "b2e54342-ad83-4944-9476-698a534ae692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "# Load the pre-trained word2vec model trained on Google News data\n",
        "# download the model and return as object ready for use\n",
        "model = api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox1KWfS1Msne"
      },
      "source": [
        "# Example of a word as a vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_AMdizk0wVA",
        "outputId": "cf6bba9d-1ee3-4cd8-b50b-a80f86032bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
            " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
            "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
            "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
            " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
            " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
            "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
            "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
            " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
            " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
            " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
            "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
            " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
            "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
            "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
            "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
            " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
            " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
            "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
            "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
            "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
            "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
            " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
            " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
            "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
            "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
            " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
            "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
            " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
            " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
            " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
            " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
            "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
            " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
            "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
            "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
            " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
            " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
            " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
            " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
            " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
            " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
            " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
            " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
            "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
            " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
            "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
            "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
            " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
            "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
            " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
            "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
            " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
            " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
            " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
            " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
            " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
            " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
            " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
            "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
            "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
            " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
            "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
            " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
            "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
            " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
            "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
            " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
            " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
            " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
            "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
            "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
            "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
            "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
            "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
          ]
        }
      ],
      "source": [
        "word_vectors = model\n",
        "\n",
        "# Lets look how the vector embedding of a word looks like\n",
        "# Accessing the vector for the word 'computer' and printing it\n",
        "print(word_vectors['computer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP4wMXZQsXyf",
        "outputId": "55003bd8-c7cb-4685-cfbf-afdd80de5044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ]
        }
      ],
      "source": [
        "print(word_vectors['cat'].shape)\n",
        "# It has 300 dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6BqaeOZM0WB"
      },
      "source": [
        "# Similar Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUPKTu3MOg8j"
      },
      "source": [
        "# King + Woman - Man = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsFqXThXXY8r"
      },
      "source": [
        "King and Man should nearly similar vectors so they cancelouts. So the output should nearly similar to the women."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtXyb8ERMyZd",
        "outputId": "9a1a385d-f45c-4fff-cef3-a3b03da76b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087411999702454)]\n"
          ]
        }
      ],
      "source": [
        "# Example of using most_similar\n",
        "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzQ2Ibo3M5DY"
      },
      "source": [
        "# The Similarity between a Few Pair of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eMx9DkoM802",
        "outputId": "65f146b1-e7f6-4617-ef9e-b63059d69875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.76640123\n",
            "0.6510957\n",
            "0.7643474\n",
            "0.8543272\n",
            "0.7594367\n",
            "0.11408084\n"
          ]
        }
      ],
      "source": [
        "# Example of calculating similarity\n",
        "print(word_vectors.similarity('woman', 'man'))\n",
        "print(word_vectors.similarity('king', 'queen'))\n",
        "print(word_vectors.similarity('uncle', 'aunt'))\n",
        "print(word_vectors.similarity('boy', 'girl'))\n",
        "print(word_vectors.similarity('nephew', 'niece'))\n",
        "print(word_vectors.similarity('paper', 'water'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clOg4fnsOIqE"
      },
      "source": [
        "# Most similar words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raqRgaROOKlb",
        "outputId": "5e7cf80f-836b-4efa-c212-b6c86b02a4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('towers', 0.8531750440597534), ('skyscraper', 0.6417425870895386), ('Tower', 0.639177143573761), ('spire', 0.594687819480896), ('responded_Understood_Atlasjet', 0.5931612253189087)]\n"
          ]
        }
      ],
      "source": [
        "print(word_vectors.most_similar(\"tower\", topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3BXCeFkMxuU"
      },
      "source": [
        "# Vector similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo0Q3I5H2naW",
        "outputId": "4ab8c06f-c488-4dff-8e05-99f1ca396e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The magnitude of the difference between 'man' and 'woman' is 1.73\n",
            "The magnitude of the difference between 'semiconductor' and 'earthworm' is 5.67\n",
            "The magnitude of the difference between 'nephew' and 'niece' is 1.96\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Words to compare\n",
        "word1 = 'man'\n",
        "word2 = 'woman'\n",
        "\n",
        "word3 = 'semiconductor'\n",
        "word4 = 'earthworm'\n",
        "\n",
        "word5 = 'nephew'\n",
        "word6 = 'niece'\n",
        "\n",
        "# Calculate the vector difference\n",
        "vector_difference1 = model[word1] - model[word2]\n",
        "vector_difference2 = model[word3] - model[word4]\n",
        "vector_difference3 = model[word5] - model[word6]\n",
        "\n",
        "# Calculate the magnitude of the vector difference\n",
        "magnitude_of_difference1 = np.linalg.norm(vector_difference1)\n",
        "magnitude_of_difference2 = np.linalg.norm(vector_difference2)\n",
        "magnitude_of_difference3 = np.linalg.norm(vector_difference3)\n",
        "\n",
        "\n",
        "# Print the magnitude of the difference\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word1, word2, magnitude_of_difference1))\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word3, word4, magnitude_of_difference2))\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word5, word6, magnitude_of_difference3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwWOT9TtWg8I"
      },
      "source": [
        "# CREATING TOKEN EMBEDDINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOODl6JBWg8I"
      },
      "source": [
        "Let's illustrate how the token ID to embedding vector conversion works with a hands-on\n",
        "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TTv14VMkWg8J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7ZA1yXLWg8J"
      },
      "source": [
        "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of\n",
        "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
        "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SsnmVW2Wg8J"
      },
      "source": [
        "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
        "setting the random seed to 123 for reproducibility purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aDSdQ2TiWg8J"
      },
      "outputs": [],
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnLDC3DwWg8J"
      },
      "source": [
        "The print statement in the code prints the embedding layer's underlying\n",
        "weight matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr1vIjPcWg8J",
        "outputId": "a8c932da-ef18-47c7-e278-7fd84ce49a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMrZmdOJWg8J"
      },
      "source": [
        "We can see that the weight matrix of the embedding layer contains small, random values.\n",
        "These values are optimized during LLM training as part of the LLM optimization itself, as we\n",
        "will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows\n",
        "and three columns. There is one row for each of the six possible tokens in the vocabulary.\n",
        "And there is one column for each of the three embedding dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0rpyDIBWg8J"
      },
      "source": [
        "   After we instantiated the embedding layer, let's now apply it to a token ID to obtain the\n",
        "embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhOCeyS4Wg8J",
        "outputId": "667534fe-fed1-426e-a37d-a277831d45d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANbc5_m-Wg8J"
      },
      "source": [
        "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we\n",
        "see that it is identical to the 4th row (Python starts with a zero index, so it's the row\n",
        "corresponding to index 3). In other words, the embedding layer is essentially a look-up\n",
        "operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVRelOs6Wg8K"
      },
      "source": [
        "    \n",
        "Previously, we have seen how to convert a single token ID into a three-dimensional\n",
        "embedding vector. Let's now apply that to all four input IDs we defined earlier\n",
        "(torch.tensor([2, 3, 5, 1])):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxQlISyGWg8K",
        "outputId": "b0b134ba-3110-4031-d00c-a22803070745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(input_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKQMOV3Wg8K"
      },
      "source": [
        "   \n",
        "Each row in this output matrix is obtained via a lookup operation from the embedding\n",
        "weight matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qa7l6RlWg8K"
      },
      "source": [
        "# POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpvZW0XaWg8K"
      },
      "source": [
        "Previously, we focused on very small embedding sizes in this chapter for illustration\n",
        "purposes.\n",
        "\n",
        "We now consider more realistic and useful embedding sizes and encode the input\n",
        "tokens into a 256-dimensional vector representation.\n",
        "\n",
        "This is smaller than what the original\n",
        "GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
        "for experimentation.\n",
        "\n",
        "Furthermore, we assume that the token IDs were created by the BPE\n",
        "tokenizer that we implemented earlier, which has a vocabulary size of 50,257"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TzTEaQCBWg8K"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCORnz4XWg8K"
      },
      "source": [
        "Using the token_embedding_layer above, if we sample data from the data loader, we\n",
        "embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8\n",
        "with four tokens each, the result will be an 8 x 4 x 256 tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyKF3otVWg8K"
      },
      "source": [
        "Let's instantiate the data loader ( Data sampling with a sliding window),\n",
        "first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx_wg0hLWg8K"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pwrgWrwWg8K",
        "outputId": "8c3a8fbf-8691-47fd-839a-38b67a02be07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgxM8-c8Wg8L"
      },
      "source": [
        "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch\n",
        "consists of 8 text samples with 4 tokens each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJDatLtzWg8L"
      },
      "source": [
        "Let's now use the embedding layer to embed these token IDs into 256-dimensional\n",
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7aJrBBFWg8L",
        "outputId": "4a285b24-cc89-4b74-8a24-820eb3631b7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9mLV9wiWg8L"
      },
      "source": [
        "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now\n",
        "embedded as a 256-dimensional vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Stu8LS5Wg8L"
      },
      "source": [
        "For a GPT model's absolute embedding approach, we just need to create another\n",
        "embedding layer that has the same dimension as the token_embedding_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wof1tutVWg8L"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrEQHj81Wg8L",
        "outputId": "d2860e8d-3f55-40bd-8237-fe777c03e015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIsspyNnWg8L"
      },
      "source": [
        "  \n",
        "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
        "placeholder vector torch.arange(context_length), which contains a sequence of\n",
        "numbers 0, 1, ..., up to the maximum input length − 1.\n",
        "\n",
        "The context_length is a variable\n",
        "that represents the supported input size of the LLM.\n",
        "\n",
        "Here, we choose it similar to the\n",
        "maximum length of the input text.\n",
        "\n",
        "In practice, input text can be longer than the supported\n",
        "context length, in which case we have to truncate the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eEXvYwMWg8L"
      },
      "source": [
        "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
        "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
        "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
        "each of the 8 batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmEvWG3MWg8M",
        "outputId": "79b73dcc-162f-4ca2-9d21-668e990073e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qi1BdnBWg8M"
      },
      "source": [
        "The input_embeddings we created are the embedded input\n",
        "examples that can now be processed by the main LLM modules"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
