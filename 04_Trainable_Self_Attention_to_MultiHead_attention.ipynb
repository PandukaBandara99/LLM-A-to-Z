{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG3uczjqx4uw"
      },
      "source": [
        "# IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "BdM7RWy2x4uw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wn4lyzix4uw"
      },
      "source": [
        "Begin by defining a few variables:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZL0KY7Mx4uw"
      },
      "source": [
        "- A The second input element\n",
        "- B The input embedding size, d=3\n",
        "- C The output embedding size, d_out=2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "sFG5s2T4x4uw"
      },
      "outputs": [],
      "source": [
        "x_2 = inputs[1] #A\n",
        "d_in = inputs.shape[1] #B\n",
        "d_out = 2 #C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGQeeDWdx4uw"
      },
      "source": [
        "Note that in GPT-like models, the input and output dimensions are usually the same.\n",
        "\n",
        "But for illustration purposes, to better follow the computation, we choose different input (d_in=3)\n",
        "and output (d_out=2) dimensions here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCuvk3pAx4uw"
      },
      "source": [
        "Initialize the three weight matrices $W_q, W_k$ and $W_v$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Query, Key, and Value** (Q, K, V)"
      ],
      "metadata": {
        "id": "rsPoifO-8JcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, the idea <br>\n",
        "\n",
        "In **attention mechanism** (like in Transformers), every word (or token) in a sentence is turned into 3 vectors\n",
        "\n",
        "* **Query (Q)** → What am I looking for?\n",
        "* **Key (K)** → What do I offer?\n",
        "* **Value (V)** → What actual information do I carry?\n",
        "\n",
        "The model uses these to decide **how much attention one word should pay to another word**\n",
        "\n",
        "**Example Sentence**\n",
        "\n",
        "> \"The cat sat on the mat.\"\n",
        "\n",
        "Let’s focus on the word **\"cat\"**.\n",
        "\n",
        "1. **Query (Q) for \"cat\"**\n",
        "\n",
        "   * Think of Query as a *question* the word asks:\n",
        "   * \"I am 'cat' — which other words are relevant to me?\"\n",
        "\n",
        "2. **Keys (K) of other words**\n",
        "\n",
        "   * Each word has a Key that represents what it *offers*.\n",
        "   * \"The\" → offers info about definiteness (the specific one).\n",
        "   * \"sat\" → offers info about action.\n",
        "   * \"mat\" → offers info about location.\n",
        "\n",
        "3. **Matching Q with K**\n",
        "\n",
        "   * We compare the **Query of 'cat'** with the **Keys of all words**.\n",
        "   * If \"cat\"’s query matches strongly with \"sat\"’s key, the model learns that \"cat\" is related to the action \"sat\".\n",
        "\n",
        "4. **Value (V)**\n",
        "\n",
        "   * Once the match is found, we use the **Value** of that word to pass the actual information.\n",
        "   * For example, if \"sat\" is important for \"cat\", the Value of \"sat\" (the action info) will be sent to update \"cat’s\" understanding.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ctvvj_P68IGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analogy: Classroom Example**\n",
        "\n",
        "* **Query (Q)** → A student asking: *\"Who knows about history?\"*\n",
        "* **Keys (K)** → Every student raises a card with what they know (\"Math\", \"History\", \"Science\").\n",
        "* **Values (V)** → The actual knowledge inside their brain (the full explanation).\n",
        "\n",
        "The asking student (Query) looks around for the right Key (\"History\") and then receives the actual Value (knowledge) from that student.\n",
        "\n",
        "So,\n",
        "* **Query = the question** (what a word is looking for)\n",
        "* **Key = the tag/label** (what a word can provide)\n",
        "* **Value = the content** (the real info that gets shared)\n"
      ],
      "metadata": {
        "id": "As9oL3_l8XVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "v5fD3eesx4uw"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "o4QCiEoDx4uw",
        "outputId": "c9fd9b10-6ab8-48f9-e50a-5e2cae2ad607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n"
          ]
        }
      ],
      "source": [
        "print(W_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "rbd4cRp_x4ux",
        "outputId": "cc12314a-f7bb-4529-afac-e3017231b6c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n"
          ]
        }
      ],
      "source": [
        "print(W_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "DQ2OEAsHx4ux",
        "outputId": "992d02fd-b450-4f21-b9aa-3c5e8e188f36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ],
      "source": [
        "print(W_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Enk3b1x4ux"
      },
      "source": [
        "Note that setting `requires_grad=False` to reduce clutter in the outputs for\n",
        "illustration purposes.\n",
        "\n",
        "If we were to use the weight matrices for model training, we\n",
        "would set `requires_grad=True` to update these matrices during model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiUtNgPqx4ux"
      },
      "source": [
        "Next, compute the query, key, and value vectors as shown earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "lZ0ACmShx4uy",
        "outputId": "af06cf46-390e-4df8-e85d-e8aee33cdda6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2uHdGvqx4uy"
      },
      "source": [
        "As we can see based on the output for the query, this results in a 2-dimensional vector.\n",
        "\n",
        "This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnpdhHzQx4uy"
      },
      "source": [
        "Even though our temporary goal is to only compute the one context vector z(2),  we still\n",
        "require the key and value vectors for all input elements.\n",
        "\n",
        "This is because they are involved in computing the attention weights with respect to the query q(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLiRILjdx4uy"
      },
      "source": [
        "We can obtain all keys and values via matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "zGta0fGLx4uy",
        "outputId": "3d9934c9-9373-4c59-8821-81edf0c28553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n",
            "queries.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "queries = inputs @ W_query\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "\n",
        "print(\"values.shape:\", values.shape)\n",
        "\n",
        "print(\"queries.shape:\", queries.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEvjQVNBx4uy"
      },
      "source": [
        "As we can tell from the outputs, we successfully projected the 6 input tokens from a 3D onto a 2D embedding space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrp7UpOXx4uy"
      },
      "source": [
        "First,compute the attention score $ω_{22}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "N_AowSJwx4uy",
        "outputId": "dd68de9d-51a4-4fdb-91c3-e000d579b811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1] #A\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpy27cOlx4u2"
      },
      "source": [
        "gain, we can generalize this computation to all attention scores via matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "b0150JF-x4u2",
        "outputId": "9843b8e6-77d7-4c34-aec1-513f43d26a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "nI2vsWoex4u2",
        "outputId": "b40b75b3-d27e-4880-d5f9-8dfb8bf561d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
            "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
            "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
            "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
            "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = queries @ keys.T # omega\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr5DrQfAx4u2"
      },
      "source": [
        "We compute the attention weights by scaling the attention scores and using the softmax function we used earlier.\n",
        "\n",
        "The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension of the keys.\n",
        "\n",
        "Note that taking the square root is mathematically the same as exponentiating by 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "_31UW1ekx4u2",
        "outputId": "939c9340-97d0-4db9-9974-dccad338c331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)\n",
        "print(d_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9E2MAlDx4u2"
      },
      "source": [
        "## WHY DIVIDE BY SQRT (DIMENSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTX5tKJbx4u2"
      },
      "source": [
        "**Reason 1: For stability in learning**\n",
        "\n",
        "The softmax function is sensitive to the magnitudes of its inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky,\" where the highest value receives almost all the probability mass, and the rest receive very little.\n",
        "\n",
        "In attention mechanisms, particularly in transformers, if the dot products between query and key vectors become too large (like multiplying by 8 in this example), the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "fGnDGBDvx4u3",
        "outputId": "f7e198c0-c8d2-4078-9dc3-5d801cafb340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "Softmax after scaling (tensor * 8): tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define the tensor\n",
        "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
        "\n",
        "# Apply softmax without scaling\n",
        "softmax_result = torch.softmax(tensor, dim=-1)\n",
        "print(\"Softmax without scaling:\", softmax_result)\n",
        "\n",
        "# Multiply the tensor by 8 and then apply softmax\n",
        "scaled_tensor = tensor * 8\n",
        "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
        "print(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iTv45QOx4u3"
      },
      "source": [
        "### BUT WHY SQRT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljwd8iCMx4u3"
      },
      "source": [
        "**Reason 2: To make the variance of the dot product stable**\n",
        "\n",
        "The dot product of  Q and K increases the variance because multiplying two random numbers increases the variance.\n",
        "\n",
        "The increase in variance grows with the dimension.\n",
        "\n",
        "Dividing by sqrt (dimension) keeps the variance close to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "Z_5z9XgNx4u3",
        "outputId": "e74e083c-ba3a-4aa6-902a-713c5bc30343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance before scaling (dim=5): 4.82439987680771\n",
            "Variance after scaling (dim=5): 0.9648799753615419\n",
            "Variance before scaling (dim=100): 99.7773104183181\n",
            "Variance after scaling (dim=100): 0.997773104183181\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to compute variance before and after scaling\n",
        "def compute_variance(dim, num_trials=1000):\n",
        "    dot_products = []\n",
        "    scaled_dot_products = []\n",
        "\n",
        "    # Generate multiple random vectors and compute dot products\n",
        "    for _ in range(num_trials):\n",
        "        q = np.random.randn(dim)\n",
        "        k = np.random.randn(dim)\n",
        "\n",
        "        # Compute dot product\n",
        "        dot_product = np.dot(q, k)\n",
        "        dot_products.append(dot_product)\n",
        "\n",
        "        # Scale the dot product by sqrt(dim)\n",
        "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
        "        scaled_dot_products.append(scaled_dot_product)\n",
        "\n",
        "    # Calculate variance of the dot products\n",
        "    variance_before_scaling = np.var(dot_products)\n",
        "    variance_after_scaling = np.var(scaled_dot_products)\n",
        "\n",
        "    return variance_before_scaling, variance_after_scaling\n",
        "\n",
        "# For dimension 5\n",
        "variance_before_5, variance_after_5 = compute_variance(5)\n",
        "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
        "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
        "\n",
        "# For dimension 20\n",
        "variance_before_100, variance_after_100 = compute_variance(100)\n",
        "print(f\"Variance before scaling (dim=100): {variance_before_100}\")\n",
        "print(f\"Variance after scaling (dim=100): {variance_after_100}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We1xeM9ox4u3"
      },
      "source": [
        "We now compute the context vector as a weighted sum over the value\n",
        "vectors.\n",
        "\n",
        "Here, the attention weights serve as a weighting factor that weighs the respective\n",
        "importance of each value vector.\n",
        "\n",
        "We can use matrix multiplication to\n",
        "obtain the output in one step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "EsbTKppnx4u3",
        "outputId": "d76c1b00-363f-4a7b-e954-f7111e1eea88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P9iTo-0x4u3"
      },
      "source": [
        "So far, we only computed a single context vector, z(2).\n",
        "\n",
        "In the next section, we will generalize the code to compute all context vectors in the input sequence, z(1)to z (T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DySh_Xr5x4u4"
      },
      "source": [
        "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W21BTd7Jx4u4"
      },
      "source": [
        "In the previous sections, we have gone through a lot of steps to compute the self-attention\n",
        "outputs.\n",
        "\n",
        "This was mainly done for illustration purposes so we could go through one step at\n",
        "a time.\n",
        "\n",
        "In practice, with the LLM implementation in the next chapter in mind, it is helpful to\n",
        "organize this code into a Python class as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "6kQE_sSRx4u4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxBj4C4Rx4u4"
      },
      "source": [
        "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a\n",
        "fundamental building block of PyTorch models, which provides necessary functionalities for\n",
        "model layer creation and management.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t548zyjJx4u4"
      },
      "source": [
        "The `__init__` method initializes trainable weight matrices (W_query, W_key, and\n",
        "W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUe9xWu4x4u4"
      },
      "source": [
        "During the forward pass, using the forward method, we compute the attention scores\n",
        "(attn_scores) by multiplying queries and keys, normalizing these scores using softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vvh-ZBax4u4"
      },
      "source": [
        "Finally, we create a context vector by weighting the values with these normalized attention scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "-PCnGo94x4u4",
        "outputId": "718d7d72-d4e9-4549-83f1-710254807d7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKkZ9WfVx4u5"
      },
      "source": [
        "Since inputs contains six embedding vectors, we get a matrix storing the six\n",
        "context vectors, as shown in the above result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NcHXXgtx4u5"
      },
      "source": [
        "As a quick check, notice how the second row ([0.3061, 0.8210]) matches the contents of context_vec_2 in the previous section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MniGih-hx4u5"
      },
      "source": [
        "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's\n",
        "nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e42qTlhx4u5"
      },
      "source": [
        "Additionally, a significant advantage of using nn.Linear instead of manually\n",
        "implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Ic9YAAUax4u5"
      },
      "outputs": [],
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZBenOMqx4u5"
      },
      "source": [
        "You can use the SelfAttention_v2 similar to SelfAttention_v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "T0jrpVofx4u5",
        "outputId": "2ef7b847-406f-4971-bdf1-5e78bb2d57c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSAiYdT_x4u6"
      },
      "source": [
        "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they use different initial weights for the weight matrices since nn.Linear uses a more sophisticated weight initialization scheme."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9zmqypZx4u6"
      },
      "source": [
        "## HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrT8vP3Gx4u6"
      },
      "source": [
        "Let's work with the attention scores and weights from the previous section to code the causal attention mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EhW-B_fx4u6"
      },
      "source": [
        "Reuse the query and key weight matrices of the SelfAttention_v2 object from the previous section for convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "FnmNSWR9x4u6",
        "outputId": "b97343ad-2d86-43cf-cf8c-b2ea693913c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "queries = sa_v2.W_query(inputs) #A\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0osFvCpx4u6"
      },
      "source": [
        "We can now use PyTorch's tril function to create a mask where the values above the diagonal are zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "aeZYJmOxx4u7",
        "outputId": "817d8a0a-efc6-46e8-8702-c055b8a6c1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63H1f5t7x4u7"
      },
      "source": [
        "Now, we can multiply this mask with the attention weights to zero out the values above the diagonal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "ZK6V0ZiFx4u7",
        "outputId": "afcf9363-b39c-4bfa-e46a-304f274e6980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udNEfYegx4u7"
      },
      "source": [
        "As we can see, the elements above the diagonal are successfully zeroed out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNtpDagRx4u7"
      },
      "source": [
        "The third step is to renormalize the attention weights to sum up to 1 again in\n",
        "each row.\n",
        "\n",
        "We can achieve this by dividing each element in each row by the sum in each\n",
        "row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "eT-vtF4lx4u7",
        "outputId": "d15d3b10-4265-4a46-8527-54d50b1f453e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDNKbfmsx4u7"
      },
      "source": [
        "The result is an attention weight matrix where the attention weights above the diagonal are zeroed out and where the rows sum to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_7Kbw94x4u8"
      },
      "source": [
        "While we could be technically done with implementing causal attention at this point, we can take advantage of a mathematical property of the softmax function.\n",
        "\n",
        "We can implement the computation of the masked attention weights more efficiently in fewer steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuG60p0Mx4u8"
      },
      "source": [
        "The softmax function converts its inputs into a probability distribution.\n",
        "\n",
        "When negative infinity values (-∞) are present in a row, the softmax function treats them as zero probability.\n",
        "\n",
        "(Mathematically, this is because e -∞ approaches 0.)\n",
        "\n",
        "\n",
        "We can implement this more efficient masking \"trick\" by creating a mask with 1's above the diagonal and then replacing these 1's with negative infinity (-inf) values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "F4iyz7NZx4u8",
        "outputId": "58c2302f-22b0-46c8-d421-41a26e209905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHh83ILQx4u8"
      },
      "source": [
        "Now, all we need to do is apply the softmax function to these masked results, and we are done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "jiWj66Lkx4u9",
        "outputId": "62b13743-0d88-4614-bc9b-4fcfcf171ed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrG8k_fPx4u9"
      },
      "source": [
        "As we can see based on the output, the values in each row sum to 1, and no further normalization is necessary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EznxTdJx4u9"
      },
      "source": [
        "Masking in Transformers sets scores for future tokens to a large negative value, making their influence in the softmax calculation effectively zero.\n",
        "\n",
        "The softmax function then recalculates attention weights only among the unmasked tokens.\n",
        "\n",
        "This process ensures no information leakage from masked tokens, focusing the model solely on the intended data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0aFJnvBx4u9"
      },
      "source": [
        "We could now use the modified attention weights to compute the context vectors via\n",
        "$$context\\_vec = attn\\_weights @ values$$\n",
        "\n",
        "However, in the next section,\n",
        "we first cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY8_PzRvx4u9"
      },
      "source": [
        "### MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ertnkA9Qx4u9"
      },
      "source": [
        "In the following code example, we use a dropout rate of 50%, which means masking out half of the attention weights.\n",
        "\n",
        "When we train the GPT model in later chapters, we will use a\n",
        "lower dropout rate, such as 0.1 or 0.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT_O5Mmjx4u9"
      },
      "source": [
        "n the following code, we apply PyTorch's dropout implementation first to a 6×6 tensor consisting of ones for illustration purposes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "2js4Glbqx4u9",
        "outputId": "64af4a22-b0b2-4cc2-aaff-496fe27ca00b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) #A\n",
        "example = torch.ones(6, 6) #B\n",
        "print(dropout(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJO-C0vHx4u-"
      },
      "source": [
        "When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero.\n",
        "\n",
        "To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 =2.\n",
        "\n",
        "This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kUBtNIqx4u-"
      },
      "source": [
        "Now, apply dropout to the attention weight matrix itself:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "3sDxSRDcx4u-",
        "outputId": "d66e46e8-e323-4826-a5be-83d7c6034861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3a2pZQXx4u-"
      },
      "source": [
        "As we can see above, the resulting attention weight matrix now has additional elements zeroed out and the remaining ones rescaled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysqvTX8Zx4u-"
      },
      "source": [
        "Having gained an understanding of causal attention and dropout masking, we will\n",
        "develop a concise Python class in the following section.\n",
        "\n",
        "This class is designed to facilitate the efficient application of these two techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV6VUSEhx4u-"
      },
      "source": [
        "### IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcGokop6x4u-"
      },
      "source": [
        "In this section, we will now incorporate the causal attention and dropout modifications into the SelfAttention Python class we developed in section 3.4.\n",
        "\n",
        "This class will then serve as a template for developing multi-head attention in the upcoming section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIrWo6YTx4u-"
      },
      "source": [
        "Before we begin, one more thing is to ensure that the code can handle batches\n",
        "consisting of more than one input.\n",
        "\n",
        "This will ensure that the CausalAttention class supports the batch\n",
        "outputs produced by the data loader we implemented earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iBASqwJx4u_"
      },
      "source": [
        "For simplicity, to simulate such batch inputs, we duplicate the input text example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF7Ks3dQx4u_"
      },
      "source": [
        "2 inputs with 6 tokens each, and each token has embedding dimension 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "qUhHcwWjx4u_",
        "outputId": "c17c27cd-ceb3-4a31-fd63-39fe66ab582e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjeV1zwwx4u_"
      },
      "source": [
        "This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token is a 3-dimensional embedding vector.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKqDpkOux4u_"
      },
      "source": [
        "The following CausalAttention class is similar to the SelfAttention class we\n",
        "implemented earlier, except that we now added the dropout and causal mask components as highlighted in the following code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwmT15bGx4u_"
      },
      "source": [
        "Step 1: Compared to the previous SelfAttention_v1 class, we added a dropout layer.\n",
        "    \n",
        "Step 2: The register_buffer call is also a new addition (more information is provided in the following text).\n",
        "\n",
        "Step 3:  We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n",
        "\n",
        "Step 4: In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory\n",
        "copies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Zp6sC82Yx4u_"
      },
      "outputs": [],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slB0hIhix4u_"
      },
      "source": [
        "The use of register_buffer in PyTorch is not strictly necessary for all use cases but offers several advantages here.\n",
        "\n",
        "For instance, when we use the CausalAttention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training the LLM in future chapters.\n",
        "\n",
        "This means we don't need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2OcKYIYx4vA"
      },
      "source": [
        "We can use the CausalAttention class as follows, similar to SelfAttention previously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "_UiSdRGvx4vA",
        "outputId": "691f8d1e-bd69-4d83-8b72-a03a22f25b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYT7LmJx4vA"
      },
      "source": [
        "As we can see, the resulting context vector is a 3D tensor where each token is now represented by a 2D embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypAR2rEmx4vB"
      },
      "source": [
        "In the next section, we will expand on this concept and implement a multi-head attention module, that implements several of such causal attention mechanisms in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrboTKDRx4vB"
      },
      "source": [
        "# EXTENDING SINGLE HEAD ATTENTION TO MULTI-HEAD ATTENTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaeKqJjgx4vB"
      },
      "source": [
        "In practical terms, implementing multi-head attention involves creating multiple instances\n",
        "of the self-attention mechanism, each with\n",
        "its own weights, and then combining their outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrcWJZJyx4vB"
      },
      "source": [
        "In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper\n",
        "class that stacks multiple instances of our previously implemented CausalAttention\n",
        "module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Z0PkXH5-x4vB"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_pTDjQVx4vB"
      },
      "source": [
        "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via\n",
        "num_heads=2) and CausalAttention output dimension d_out=2, this results in a 4-\n",
        "dimensional context vectors (d_out*num_heads=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzWIoFcOx4vB"
      },
      "source": [
        "To illustrate further with a concrete example, we can use the\n",
        "MultiHeadAttentionWrapper class similar to the CausalAttention class before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "6zg2qHF6x4vB",
        "outputId": "6ae962ab-9c4a-487d-eac2-94116c3e912a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1] # This is the number of tokens\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJV07NUcx4vC"
      },
      "source": [
        "The first dimension of the resulting context_vecs tensor is 2 since we have two input texts\n",
        "(the input texts are duplicated, which is why the context vectors are exactly the same for\n",
        "those).\n",
        "\n",
        "The second dimension refers to the 6 tokens in each input. The third dimension\n",
        "refers to the 4-dimensional embedding of each token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoWP_MCQx4vC"
      },
      "source": [
        "In this section, we implemented a MultiHeadAttentionWrapper that combined multiple\n",
        "single-head attention modules.\n",
        "\n",
        "However, note that these are processed sequentially via\n",
        "[head(x) for head in self.heads] in the forward method.\n",
        "\n",
        "We can improve this\n",
        "implementation by processing the heads in parallel.\n",
        "\n",
        "One way to achieve this is by\n",
        "computing the outputs for all attention heads simultaneously via matrix multiplication, as\n",
        "we will explore in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St3iIK7Hx4vC"
      },
      "source": [
        "### IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksYIhDSx4vC"
      },
      "source": [
        " Instead of maintaining two separate classes, MultiHeadAttentionWrapper and\n",
        "CausalAttention, we can combine both of these concepts into a single\n",
        "MultiHeadAttention class.\n",
        "\n",
        "Also, in addition to just merging the\n",
        "MultiHeadAttentionWrapper with the CausalAttention code, we will make some other\n",
        "modifications to implement multi-head attention more efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zLpUWqdx4vC"
      },
      "source": [
        "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list\n",
        "of CausalAttention objects (self.heads), each representing a separate attention head.\n",
        "\n",
        "\n",
        "The CausalAttention class independently performs the attention mechanism, and the\n",
        "results from each head are concatenated.\n",
        "\n",
        "In contrast, the following MultiHeadAttention\n",
        "class integrates the multi-head functionality within a single class.\n",
        "\n",
        "\n",
        "It splits the input into\n",
        "multiple heads by reshaping the projected query, key, and value tensors and then combines\n",
        "the results from these heads after computing attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMwVoJYXx4vC"
      },
      "source": [
        "Let's take a look at the MultiHeadAttention class before we discuss it further"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "ubwA7w1Mx4vD"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36rfLTT1x4vD"
      },
      "source": [
        "Step 1: Reduce the projection dim to match desired output dim\n",
        "\n",
        "Step 2: Use a Linear layer to combine head outputs\n",
        "\n",
        "Step 3: Tensor shape: (b, num_tokens, d_out)\n",
        "\n",
        "Step 4: We implicitly split the matrix by adding a `num_heads` dimension. Then we unroll last dim: (b,\n",
        "num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "\n",
        "Step 5: Transpose from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
        "\n",
        "Step 6: Compute dot product for each head\n",
        "\n",
        "Step 7: Mask truncated to the number of tokens\n",
        "\n",
        "Step 8: Use the mask to fill attention scores\n",
        "\n",
        "Step 9: Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
        "\n",
        "Step 10: Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "\n",
        "Step 11: Add an optional linear projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TxpCfjvx4vD"
      },
      "source": [
        "Even though the reshaping (.view) and transposing (.transpose) of tensors inside the\n",
        "MultiHeadAttention class looks very complicated, mathematically, the\n",
        "MultiHeadAttention class implements the same concept as the\n",
        "MultiHeadAttentionWrapper earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e73rFF6Cx4vD"
      },
      "source": [
        "On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked\n",
        "multiple single-head attention layers that we combined into a multi-head attention layer.\n",
        "\n",
        "\n",
        "The MultiHeadAttention class takes an integrated approach.\n",
        "\n",
        "It starts with a multi-head\n",
        "layer and then internally splits this layer into individual attention heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiiiMRidx4vD"
      },
      "source": [
        "#### DETAILED EXPLANATION OF THE MULTI-HEAD ATTENTION CLASS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4LBuRVFx4vD"
      },
      "source": [
        "The splitting of the query, key, and value tensors, is achieved\n",
        "through tensor reshaping and transposing operations using PyTorch's .view and\n",
        ".transpose methods.\n",
        "\n",
        "The input is first transformed (via linear layers for queries, keys, and\n",
        "values) and then reshaped to represent multiple heads.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROiGaIENx4vD"
      },
      "source": [
        "The key operation is to split the d_out dimension into num_heads and head_dim, where\n",
        "head_dim = d_out / num_heads.\n",
        "\n",
        "This splitting is then achieved using the .view method: a\n",
        "tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens,\n",
        "num_heads, head_dim)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nNwdc3Ux4vE"
      },
      "source": [
        "The tensors are then transposed to bring the num_heads dimension before the\n",
        "num_tokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim).\n",
        "\n",
        "This transposition is crucial for correctly aligning the queries, keys, and values across the\n",
        "different heads and performing batched matrix multiplications efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvtSXNT9x4vE"
      },
      "source": [
        "To illustrate this batched matrix multiplication, suppose we have the following example\n",
        "tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "PXblcN05x4vE"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n",
        "[0.8993, 0.0390, 0.9268, 0.7388],\n",
        "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "[0.4066, 0.2318, 0.4545, 0.9737],\n",
        "[0.4606, 0.5159, 0.4220, 0.5786]]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klRVuuYEx4vE"
      },
      "source": [
        "The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQZ3zEwjx4vE"
      },
      "source": [
        "Now, we perform a batched matrix multiplication between the tensor itself and a view of\n",
        "the tensor where we transposed the last two dimensions, num_tokens and head_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "a7e1RJlBx4vE",
        "outputId": "f2fb8c53-88a3-46de-bfa3-60aebab878de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ],
      "source": [
        "print(a @ a.transpose(2, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yJf_MdNx4vE"
      },
      "source": [
        "In this case, the matrix multiplication implementation in PyTorch handles the 4-dimensional\n",
        "input tensor so that the matrix multiplication is carried out between the 2 last dimensions\n",
        "(num_tokens, head_dim) and then repeated for the individual heads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a32M4A0ax4vE"
      },
      "source": [
        "For instance, the above becomes a more compact way to compute the matrix\n",
        "multiplication for each head separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "e9J1rIRLx4vE",
        "outputId": "b1d418a1-1c24-400c-d3e4-8a4752da1e4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First head:\n",
            " tensor([[1.3208, 1.1631, 1.2879],\n",
            "        [1.1631, 2.2150, 1.8424],\n",
            "        [1.2879, 1.8424, 2.0402]])\n",
            "\n",
            "Second head:\n",
            " tensor([[0.4391, 0.7003, 0.5903],\n",
            "        [0.7003, 1.3737, 1.0620],\n",
            "        [0.5903, 1.0620, 0.9912]])\n"
          ]
        }
      ],
      "source": [
        "first_head = a[0, 0, :, :]\n",
        "first_res = first_head @ first_head.T\n",
        "print(\"First head:\\n\", first_res)\n",
        "second_head = a[0, 1, :, :]\n",
        "second_res = second_head @ second_head.T\n",
        "print(\"\\nSecond head:\\n\", second_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmfgMqg5x4vE"
      },
      "source": [
        "The results are exactly the same results that we obtained when using the batched matrix\n",
        "multiplication print(a @ a.transpose(2, 3)) earlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx5G4rMKx4vE"
      },
      "source": [
        "Continuing with MultiHeadAttention, after computing the attention weights and context\n",
        "vectors, the context vectors from all heads are transposed back to the shape (b,\n",
        "num_tokens, num_heads, head_dim).\n",
        "\n",
        "These vectors are then reshaped (flattened) into the\n",
        "shape (b, num_tokens, d_out), effectively combining the outputs from all heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSxHFAQ6x4vE"
      },
      "source": [
        "Additionally, we added a so-called output projection layer (self.out_proj) to\n",
        "MultiHeadAttention after combining the heads, which is not present in the\n",
        "CausalAttention class.\n",
        "\n",
        "This output projection layer is not strictly necessary, but it is commonly used in many LLM\n",
        "architectures, which is why we added it here for completeness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7YKW4ZJx4vF"
      },
      "source": [
        "Even though the MultiHeadAttention class looks more complicated than the\n",
        "MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors,\n",
        "it is more efficient.\n",
        "\n",
        "The reason is that we only need one matrix multiplication to compute\n",
        "the keys, for instance, keys = self.W_key(x) (the same is true for the queries and\n",
        "values).\n",
        "                                              \n",
        "\n",
        "In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication,\n",
        "which is computationally one of the most expensive steps, for each attention head.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unHNVYPrx4vF"
      },
      "source": [
        "The MultiHeadAttention class can be used similar to the SelfAttention and\n",
        "CausalAttention classes we implemented earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "CKgNzyJUx4vF",
        "outputId": "ace21d20-b94f-4350-fdf0-44081e5aefb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL30k2D5x4vF"
      },
      "source": [
        "As we can see based on the results, the output dimension is directly controlled by the\n",
        "d_out argument"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvPjBZxjx4vF"
      },
      "source": [
        "For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention\n",
        "heads and a context vector embedding size of 768.\n",
        "\n",
        "The largest GPT-2 model (1.5 billion\n",
        "parameters) has 25 attention heads and a context vector embedding size of 1600.\n",
        "\n",
        "Note\n",
        "that the embedding sizes of the token inputs and context embeddings are the same in GPT\n",
        "models (d_in = d_out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "T22ekextx4vF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}